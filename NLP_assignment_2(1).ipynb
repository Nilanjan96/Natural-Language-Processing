{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lTzbbaKDUGG"
      },
      "source": [
        "#Nilanjan Debnath MDS201919 Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LChITkmqu-wH"
      },
      "source": [
        "Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOcqjEKvzC6d"
      },
      "source": [
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "import time\n",
        "import json\n",
        "from nltk.util import ngrams\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCNDw5Zi1iv3",
        "outputId": "d0a56cd4-7b06-4030-fbad-fb105c568523"
      },
      "source": [
        "cd '/content/drive/MyDrive/data'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulSaRN_wvCU_"
      },
      "source": [
        "Reading our corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usH-NdrZ0d5y"
      },
      "source": [
        "corpus = open('corpus2.txt','r')\n",
        "corpus1 = corpus.readlines()\n",
        "corpus.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi_nsG2l2SVn"
      },
      "source": [
        "Number of documents : 11003534"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbPg2RgQ39Gh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de76777-a7a0-43ac-c1c7-8bb9b686985a"
      },
      "source": [
        "len(corpus1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11003534"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rs2NvkAvFSP"
      },
      "source": [
        "Our trigram model will be stored here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8D9Li272QM4"
      },
      "source": [
        "model = defaultdict(lambda: defaultdict(lambda: 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNj83H5MvHyn"
      },
      "source": [
        "We use the first 10 lakh documents for this purpose because of computational problems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpokYArh2aQ9",
        "outputId": "be4eb8d7-f47d-4464-9bbb-19f04c0021c7"
      },
      "source": [
        "startTime = time.time()\n",
        "j = 1\n",
        "for i in tqdm(range(1000000)):\n",
        "  sentence = corpus1[i].strip()\n",
        "  sentence = sentence.replace('\\n', '')\n",
        "  for w1, w2, w3 in list(ngrams(sentence.split(\" \"),3, pad_right=True, pad_left=True)):\n",
        "        #if((w1,w2) in model):\n",
        "          #if((w1,w2)[w3] in model):\n",
        "            #model[(w1,w2)][w3] += 1\n",
        "          #else:\n",
        "            #model[(w1,w2)][w3] = 1\n",
        "        #else:\n",
        "          #model[(w1,w2)] = {w3:1}\n",
        "    model[(w1, w2)][w3] += 1\n",
        "  #print(j)\n",
        "  #j += 1\n",
        "executionTime = (time.time() - startTime)\n",
        "print('Execution time in seconds: ' + str(executionTime))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000000/1000000 [01:03<00:00, 15711.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time in seconds: 64.01607275009155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keUVDL1tvO2H"
      },
      "source": [
        "Calculating the probabilities of each (key,value) pair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4MXW9Km2zzv"
      },
      "source": [
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgfzY_MGvVRv"
      },
      "source": [
        "The most starting word for the trigram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pQK9I5jRzT37",
        "outputId": "1f619504-2df6-4f0f-b095-246cf12e0044"
      },
      "source": [
        "max(model[None,None], key=model[None,None].get)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QGqkilcyX3Q"
      },
      "source": [
        "Creating a sentence with the most occuring starting word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCIFJJKh7JJD",
        "outputId": "c1b2174c-9cd2-4a8b-e761-2df50ab08b2e"
      },
      "source": [
        "startTime = time.time()\n",
        "text = [None,\"The\"] #starting with [None, \"The\"]\n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "  old_prob = .0\n",
        "  next_word = ''\n",
        "  for word in model[tuple(text[-2:])].keys():\n",
        "      new_prob = model[tuple(text[-2:])][word]\n",
        "      if new_prob >= old_prob:\n",
        "          #text.append(word)\n",
        "          #break\n",
        "          old_prob = new_prob\n",
        "          next_word = word\n",
        "  text.append(next_word)\n",
        "  #print(next_word)\n",
        "  #text.append(max(model, key= lambda x: model[x]) )\n",
        "\n",
        "  if text[-2:] == [None, None]:\n",
        "      sentence_finished = True\n",
        " \n",
        "print (' '.join([t for t in text if t]))\n",
        "executionTime = (time.time() - startTime)\n",
        "print('Execution time in seconds: ' + str(executionTime))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The copyright holder for this preprint this version posted June 5, 2020\n",
            "Execution time in seconds: 0.016626596450805664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PhzPaqbzHg3"
      },
      "source": [
        "Model for the 4-gram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPGnaMVFIMDp"
      },
      "source": [
        "model2 = defaultdict(lambda: defaultdict(lambda: 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCT965pP0sG8"
      },
      "source": [
        "Training over 10 lakh documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAi9F30c7T6G",
        "outputId": "fac21693-041e-424c-ac31-fb1f360aff17"
      },
      "source": [
        "startTime = time.time()\n",
        "j = 1\n",
        "for i in tqdm(range(1000000)):\n",
        "  sentence = corpus1[i].strip()\n",
        "  sentence = sentence.replace('\\n', '')\n",
        "  for w1, w2, w3, w4 in list(ngrams(sentence.split(\" \"),4, pad_right=True, pad_left=True)):\n",
        "        #if((w1,w2) in model):\n",
        "          #if((w1,w2)[w3] in model):\n",
        "            #model[(w1,w2)][w3] += 1\n",
        "          #else:\n",
        "            #model[(w1,w2)][w3] = 1\n",
        "        #else:\n",
        "          #model[(w1,w2)] = {w3:1}\n",
        "    model2[(w1, w2, w3)][w4] += 1\n",
        "  #print(j)\n",
        "  #j += 1\n",
        "executionTime = (time.time() - startTime)\n",
        "print('Execution time in seconds: ' + str(executionTime))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000000/1000000 [01:22<00:00, 12130.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Execution time in seconds: 82.44127297401428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFRsj3K17dN7"
      },
      "source": [
        "for w1_w2_w3 in model2:\n",
        "    total_count = float(sum(model2[w1_w2_w3].values()))\n",
        "    for w4 in model2[w1_w2_w3]:\n",
        "        model2[w1_w2_w3][w4] /= total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91C_Iku10vbl"
      },
      "source": [
        "The most starting word for the trigram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "f3zmMp2L8ZHf",
        "outputId": "e85e16da-8bf1-4a58-e28d-cee02ecfaa65"
      },
      "source": [
        "max(model2[None,None,None], key=model2[None,None,None].get)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsZxCHY0011N"
      },
      "source": [
        "Creating a sentence with the most occuring starting word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zeqwbJ_8ica",
        "outputId": "cdb97490-6ea4-4994-f827-0a4dac329f5b"
      },
      "source": [
        "startTime = time.time()\n",
        "text2 = [None,None,\"The\"]\n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "  old_prob = .0\n",
        "  next_word = ''\n",
        "  for word in model2[tuple(text2[-3:])].keys():\n",
        "      new_prob = model2[tuple(text2[-3:])][word]\n",
        "      if new_prob >= old_prob:\n",
        "          #text.append(word)\n",
        "          #break\n",
        "          old_prob = new_prob\n",
        "          next_word = word\n",
        "  text2.append(next_word)\n",
        "  #print(next_word)\n",
        "  #text.append(max(model, key= lambda x: model[x]) )\n",
        "\n",
        "  if text2[-3:] == [None, None,None]:\n",
        "      sentence_finished = True\n",
        " \n",
        "print (' '.join([t for t in text2 if t]))\n",
        "executionTime = (time.time() - startTime)\n",
        "print('Execution time in seconds: ' + str(executionTime))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The copyright holder for this preprint this version posted June 5, 2020\n",
            "Execution time in seconds: 0.01645636558532715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50GatBQE03Ou"
      },
      "source": [
        "Creating the test dataset of 1 lakh documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj9Tr27C96lr"
      },
      "source": [
        "test = corpus1[1000000:1100000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JeRDY120711"
      },
      "source": [
        "We calculate the average probability of the model estimating the sentence. \n",
        "Given a test sentence, we first take all the ngrams of the sentence and multiplyt the probability of each from our model. For each ngram which is not available, we multiply a probability of $10^{-6}$. The we average out the probability of all documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id54xujJn13O",
        "outputId": "57b001d1-ef8c-4af6-af51-1e5cdffeb72e"
      },
      "source": [
        "start = time.time()\n",
        "avg_prob = 0.0\n",
        "for i in tqdm(test):\n",
        "  sentence = i.strip()\n",
        "  sentence = sentence.replace('\\n','')\n",
        "  prob_sentence = 1.0\n",
        "  for w1, w2, w3 in list(ngrams(sentence.split(\" \"),3, pad_right=True, pad_left=True)):\n",
        "    prob = model[w1,w2][w3]\n",
        "    if(prob>0):\n",
        "      prob_sentence = prob_sentence*prob\n",
        "    else:\n",
        "      prob_sentence = prob_sentence*(10**-6)\n",
        "  avg_prob = avg_prob + prob_sentence\n",
        "print('\\n')\n",
        "print(avg_prob/100000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:04<00:00, 20851.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "0.00010522992083804258\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJrPyrt3vuSK",
        "outputId": "d445c689-e3f5-4bc8-fcb8-f889ed0c5a4c"
      },
      "source": [
        "start = time.time()\n",
        "avg_prob = 0.0\n",
        "for i in tqdm(test):\n",
        "  sentence = i.strip()\n",
        "  sentence = sentence.replace('\\n','')\n",
        "  prob_sentence = 1.0\n",
        "  for w1, w2, w3, w4 in list(ngrams(sentence.split(\" \"),4, pad_right=True, pad_left=True)):\n",
        "    prob = model2[w1,w2,w3][w4]\n",
        "    if(prob>0):\n",
        "      prob_sentence = prob_sentence*prob\n",
        "    else:\n",
        "      prob_sentence = prob_sentence*(10**-6)\n",
        "  avg_prob = avg_prob + prob_sentence\n",
        "print('\\n')\n",
        "print(avg_prob/100000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:06<00:00, 15871.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "0.00010870750562868375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mK14R3G6_-D"
      },
      "source": [
        "We get a better probability for 4gram rather than trigrams. Hence 4gram model is a better model than trigram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUOAhjX9sWVm"
      },
      "source": [
        "#Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGjTM9TQtnBR"
      },
      "source": [
        "Since I used a dictionary within a dictionary, it's already quite a efficient way to handle the parameters. Another strategy which can be useful is that every 3 word (or 4 word) combinations can take up a lot of space. So we could replace them with integer keys and keep the key list as one. Since the english vocabulary is around 1.8 lakh, (and in a medical dataset, most of it will not be used), it makes sense to use this strategy. For a small dataset it will be computationally problematic and profits will be limited, however for a large enough dataset, even though it is problematic, the amount of profit overturns the amount of problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSEx6Pqouw-A"
      },
      "source": [
        "#Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAyM5mY5u0EH"
      },
      "source": [
        "It is possible to run part of the code in parallel. When we are training the model, we iterate over 10 lakh documents to train it on the same dictionary. We can divide it into 2 or 3 parts and create 3 different dictionaries. Then we can just add the number of trigrams(or 4grams), find the probability, and get the final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfNgWo45pRGN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}